# 7.3) Audnexus Source (`sources/audnexus.py`)

## Overview

The Audnexus Source provides robust integration with the Audnexus API (api.audnex.us) for authoritative audiobook metadata. It handles ASIN extraction, API communication, response normalization, and error recovery with **comprehensive enhanced field support**.

**Enhanced Field Coverage**: Supports all enhanced fields from the canonical data model including `subtitle`, `copyright`, `release_date`, `rating`, `region`, `literature_type`, `format_type`, `is_adult`, `description_html`, and `description_text`.

## Features

### ASIN Extraction

- **Filename parsing**: Extract ASINs from standardized filenames
- **Embedded tag reading**: Read ASINs from audio file metadata
- **Multiple patterns**: Support various ASIN formats and positions
- **Validation**: Verify ASIN format and checksums

### API Integration

- **HTTP client abstraction**: Pluggable backend (httpx preferred/requests fallback)
- **Rate limiting**: Configurable request throttling
- **Retry logic**: Exponential backoff with jitter
- **Timeout handling**: Configurable connection/read timeouts

### Chapter Integration

- **Comprehensive chapter support**: Integrated chapter fetching in main extract method
- **Structured chapter data**: Standardized API chapter information with titles and timing
- **Graceful degradation**: Chapter extraction never fails main metadata operation
- **Enhanced timing**: Uses chapter runtime when more accurate than book runtime

### Response Normalization

- **Enhanced field mapping**: All API fields → canonical model with enhanced field support
- **Conflict-free descriptions**: Separate handling of `description` vs `summary` fields
- **Data type conversion**: String → int/float/date conversions with validation
- **List processing**: Authors, narrators, genres/tags normalization with type separation
- **HTML cleaning**: Description sanitization while preserving rich content

## Interface

```python
class AudnexusSource:
    def __init__(
        self,
        http_client: HTTPClient,
        rate_limit: float = 5.0,
        timeout: float = 10.0,
        max_retries: int = 3
    ):
        """
        Initialize Audnexus source.

        Args:
            http_client: HTTP client implementation
            rate_limit: Max requests per second
            timeout: Request timeout in seconds
            max_retries: Max retry attempts
        """

    def lookup(self, asin: str) -> Dict[str, Any]:
        """
        Lookup audiobook metadata by ASIN with chapter support.

        Args:
            asin: Amazon ASIN identifier

        Returns:
            Normalized metadata dictionary with chapter data
        """

        # Fetch main book metadata
        book_data = self._fetch_book_metadata(asin)
        if not book_data:
            return {}

        # Fetch chapter data (optional, best effort)
        try:
            chapter_data = self._fetch_chapter_metadata(asin)
            if chapter_data:
                # Merge chapter data into book metadata
                book_data.update(chapter_data)
        except Exception as e:
            # Log but don't fail on chapter errors
            logger.debug(f"Chapter lookup failed for {asin}: {e}")

        return book_data

    def _fetch_book_metadata(self, asin: str) -> Dict[str, Any]:
        """Fetch main book metadata from /books/{asin} endpoint."""
        # Implementation from existing lookup logic...
        pass

    def _fetch_chapter_metadata(self, asin: str) -> Dict[str, Any]:
        """Fetch chapter metadata from /books/{asin}/chapters endpoint."""
        if not self._validate_asin(asin):
            return {}

        def _api_call():
            self.rate_limiter.acquire()

            url = f"https://api.audnex.us/books/{asin}/chapters"
            response = self.http_client.get(url)

            if response.status_code == 404:
                # Chapters not available for this book
                return {}
            elif response.status_code != 200:
                raise APIUnavailable(f"Chapter API error: {response.status_code}")

            return response.json()

        try:
            raw_data = with_retries(_api_call, self.max_retries)
            if raw_data:
                return self._normalize_chapter_response(raw_data)
        except Exception as e:
            logger.debug(f"Chapter data unavailable for {asin}: {e}")

        return {}

    def extract_asin(self, source: Union[Path, str]) -> Optional[str]:
        """
        Extract ASIN from filename or path.

        Args:
            source: File path or filename string

        Returns:
            Extracted ASIN or None if not found
        """
```

## ASIN Extraction

### Pattern Matching

```python
import re

ASIN_PATTERNS = [
    r'\{([A-Z0-9]{10})\}',           # {B001234567}
    r'\{([A-Z0-9]{10})\.[A-Z]{3}\}', # {B001234567.ABC}
    r'ASIN[:\s]*([A-Z0-9]{10})',     # ASIN: B001234567
    r'B00[A-Z0-9]{7}',               # B001234567 (loose)
]

def extract_asin(self, source: Union[Path, str]) -> Optional[str]:
    """Extract ASIN using multiple patterns."""
    text = str(source)

    for pattern in ASIN_PATTERNS:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            asin = match.group(1) if match.groups() else match.group(0)
            if self._validate_asin(asin):
                return asin.upper()

    return None
```

### ASIN Validation

```python
def _validate_asin(self, asin: str) -> bool:
    """Validate ASIN format and checksum."""
    if not asin or len(asin) != 10:
        return False

    # Must start with B for books
    if not asin.startswith('B'):
        return False

    # Only alphanumeric characters
    if not re.match(r'^[A-Z0-9]{10}$', asin):
        return False

    return True
```

### Embedded Tag Extraction

```python
def _extract_from_tags(self, file_path: Path) -> Optional[str]:
    """Extract ASIN from audio file tags."""
    try:
        from mutagen import File as MutagenFile

        audio = MutagenFile(file_path)
        if not audio or not audio.tags:
            return None

        # Check common ASIN tag fields
        asin_fields = ['ASIN', 'TXXX:ASIN', 'comment', 'description']

        for field in asin_fields:
            if field in audio.tags:
                value = str(audio.tags[field][0])
                asin = self.extract_asin(value)
                if asin:
                    return asin

        return None
    except ImportError:
        # Mutagen not available
        return None
    except Exception:
        # File reading error
        return None
```

## API Communication

### HTTP Client Abstraction

```python
from abc import ABC, abstractmethod

class HTTPClient(ABC):
    @abstractmethod
    def get(self, url: str, **kwargs) -> Response:
        """Perform GET request."""
        pass

class HTTPXClient(HTTPClient):
    """Preferred HTTP client using httpx."""
    def __init__(self, timeout: float = 10.0):
        import httpx
        self.client = httpx.Client(timeout=timeout)

    def get(self, url: str, **kwargs) -> httpx.Response:
        return self.client.get(url, **kwargs)

class RequestsClient(HTTPClient):
    """Fallback HTTP client using requests."""
    def __init__(self, timeout: float = 10.0):
        import requests
        self.timeout = timeout

    def get(self, url: str, **kwargs) -> requests.Response:
        import requests
        return requests.get(url, timeout=self.timeout, **kwargs)
```

### Rate Limiting

```python
import time
from threading import Lock

class RateLimiter:
    def __init__(self, max_rate: float):
        self.max_rate = max_rate  # requests per second
        self.min_interval = 1.0 / max_rate
        self.last_request = 0.0
        self.lock = Lock()

    def acquire(self):
        """Wait for rate limit clearance."""
        with self.lock:
            now = time.time()
            elapsed = now - self.last_request

            if elapsed < self.min_interval:
                sleep_time = self.min_interval - elapsed
                time.sleep(sleep_time)

            self.last_request = time.time()
```

### Retry Logic

```python
import random
from typing import Callable, TypeVar

T = TypeVar('T')

def with_retries(
    func: Callable[[], T],
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0
) -> T:
    """Execute function with exponential backoff retries."""

    for attempt in range(max_retries + 1):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries:
                raise e

            # Exponential backoff with jitter
            delay = min(base_delay * (2 ** attempt), max_delay)
            jitter = random.uniform(0.1, 0.9) * delay
            time.sleep(jitter)

    raise RuntimeError("Unreachable")
```

## API Response Handling

### Response Structure

```python
# Example Audnexus API response (comprehensive)
SAMPLE_RESPONSE = {
    "asin": "B0C8ZW5N6Y",
    "title": "How a Realist Hero Rebuilt the Kingdom: Volume 3",
    "subtitle": "How a Realist Hero Rebuilt the Kingdom, Book 3",
    "authors": [
        {
            "asin": "B06W5GKCZW",
            "name": "Dojyomaru"
        }
    ],
    "narrators": [
        {"name": "BJ Harrison"}
    ],
    "publisherName": "Tantor Audio",
    "copyright": 2017,
    "description": "Souma presses Elfrieden's siege on the Amidonian principality, whose soldiers fight tooth and nail to keep him at bay. The violence on the front lines is bad enough, but a surprise attack at the rear will confound the bloody battle further....",
    "summary": "<p><b>The Battle Continues!</b></p> <p>Souma presses Elfrieden's siege on the Amidonian principality, whose soldiers fight tooth and nail to keep him at bay. The violence on the front lines is bad enough, but a surprise attack at the rear will confound the bloody battle further. Souma and Carla must fend off Gaius themselves in a life-or-death test of nerves, loyalty, and steel. When hard-fought victory finally dawns over the capital, it's time for the realist hero to get to work. Souma now needs to win over the citizens of the former enemy nation, and he has a most unusual plan that just might do the trick.</p>",
    "releaseDate": "2023-07-11T00:00:00.000Z",
    "runtimeLengthMin": 524,
    "formatType": "unabridged",
    "language": "english",
    "literatureType": "fiction",
    "rating": "4.8",
    "region": "us",
    "isAdult": false,
    "isbn": "9798765080221",
    "image": "https://m.media-amazon.com/images/I/81IpsoA4EqL.jpg",
    "genres": [
        {
            "asin": "18580606011",
            "name": "Science Fiction & Fantasy",
            "type": "genre"
        },
        {
            "asin": "18580607011",
            "name": "Fantasy",
            "type": "tag"
        },
        {
            "asin": "18580615011",
            "name": "Epic",
            "type": "tag"
        },
        {
            "asin": "18580618011",
            "name": "Historical",
            "type": "tag"
        }
    ],
    "seriesPrimary": {
        "asin": "B0C37XK8SV",
        "name": "How a Realist Hero Rebuilt the Kingdom",
        "position": "3"
    }
}
```

### Field Normalization

**Field Mapping Strategy**: This implementation follows the comprehensive field mapping strategy defined in **Document 04 — Canonical Data Model, Section 4.4.1**. The mapping avoids field conflicts by properly separating `description` (short, plain text) from `summary` (full, HTML) content using enhanced fields.

```python
def _normalize_response(self, raw_data: dict) -> Dict[str, Any]:
    """Normalize API response to internal format with enhanced field support."""
    normalized = {}

    # Basic string fields
    normalized['title'] = raw_data.get('title', '')
    normalized['asin'] = raw_data.get('asin', '')
    normalized['isbn'] = raw_data.get('isbn', '')

    # Enhanced fields - direct mappings
    normalized['subtitle'] = raw_data.get('subtitle', '')
    normalized['publisher'] = raw_data.get('publisherName', '')
    normalized['language'] = self._normalize_language(raw_data.get('language', ''))
    normalized['region'] = raw_data.get('region', '')
    normalized['literature_type'] = raw_data.get('literatureType', '')
    normalized['format_type'] = raw_data.get('formatType', '')
    normalized['is_adult'] = raw_data.get('isAdult', False)
    normalized['artwork_url'] = raw_data.get('image', '')

    # Copyright field (convert int to string)
    copyright_val = raw_data.get('copyright')
    if copyright_val:
        normalized['copyright'] = str(copyright_val)

    # Rating field (convert string to float)
    rating_val = raw_data.get('rating')
    if rating_val:
        try:
            normalized['rating'] = float(rating_val)
        except (ValueError, TypeError):
            pass

    # Release date (enhanced field)
    release_date = raw_data.get('releaseDate', '')
    if release_date:
        normalized['release_date'] = release_date
        normalized['year'] = self._extract_year(release_date)

    # Duration conversion (minutes to seconds)
    runtime = raw_data.get('runtimeLengthMin')
    if runtime:
        normalized['duration_sec'] = int(runtime) * 60

    # Author list processing
    authors = raw_data.get('authors', [])
    if authors:
        author_names = [author.get('name', '') for author in authors]
        normalized['author'] = ', '.join(filter(None, author_names))

    # Narrator list processing
    narrators = raw_data.get('narrators', [])
    if narrators:
        narrator_names = [narrator.get('name', '') for narrator in narrators]
        normalized['narrator'] = ', '.join(filter(None, narrator_names))

    # Description fields (CRITICAL: avoiding conflicts)
    description = raw_data.get('description', '')
    summary = raw_data.get('summary', '')

    if description:
        # Short, plain text version
        normalized['description'] = description          # Backward compatibility
        normalized['description_text'] = description     # Enhanced field (explicit)

    if summary:
        # Full, HTML version
        normalized['description_html'] = summary         # Enhanced field
        # If no short description, use cleaned HTML as fallback
        if not description:
            normalized['description'] = self._clean_html(summary)

    # Genre/tag list processing (split by type)
    genres_list = []
    tags_list = []
    genres = raw_data.get('genres', [])
    for genre in genres:
        genre_name = genre.get('name', '')
        genre_type = genre.get('type', 'genre')

        if genre_name:
            if genre_type == 'genre':
                genres_list.append(genre_name)
            else:  # 'tag' or other types
                tags_list.append(genre_name)

    normalized['genres'] = genres_list
    normalized['tags'] = tags_list

    # Series information (handle both seriesPrimary and seriesSecondary)
    series_primary = raw_data.get('seriesPrimary')
    if series_primary:
        normalized['series'] = series_primary.get('name', '')
        position = series_primary.get('position', '')
        if position:
            normalized['volume'] = str(position).zfill(2)

    return normalized

def _normalize_chapter_response(self, chapters_data: dict) -> Dict[str, Any]:
    """Normalize chapter API response to internal format."""
    normalized = {}

    # Chapter list processing
    chapters = chapters_data.get('chapters', [])
    if chapters:
        chapter_list = []
        for chapter in chapters:
            chapter_info = {
                'title': chapter.get('title', ''),
                'start_time': chapter.get('startOffsetSec', 0),
                'length': chapter.get('lengthMs', 0) / 1000.0 if chapter.get('lengthMs') else 0
            }
            chapter_list.append(chapter_info)
        normalized['chapters'] = chapter_list

    # Runtime from chapters (if available and more accurate)
    runtime_sec = chapters_data.get('runtimeLengthSec')
    if runtime_sec:
        normalized['duration_sec'] = int(runtime_sec)

    # Chapter accuracy metadata
    is_accurate = chapters_data.get('isAccurate', True)
    normalized['chapter_accuracy'] = is_accurate

    return normalized

def _normalize_language(self, language: str) -> str:
    """Normalize language names to ISO-639-1 codes."""
    if not language:
        return 'en'

    # Common language mappings
    language_map = {
        'english': 'en',
        'spanish': 'es',
        'french': 'fr',
        'german': 'de',
        'italian': 'it',
        'portuguese': 'pt',
        'japanese': 'ja',
        'chinese': 'zh',
        'korean': 'ko',
        'russian': 'ru'
    }

    lang_lower = language.lower()
    return language_map.get(lang_lower, lang_lower[:2] if len(lang_lower) >= 2 else 'en')

def _clean_html(self, html_content: str) -> str:
    """Clean HTML content to plain text."""
    if not html_content:
        return ''

    # Basic HTML stripping (use HTML Cleaner service in production)
    import re
    # Remove HTML tags
    clean_text = re.sub(r'<[^>]+>', '', html_content)
    # Decode common HTML entities
    clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>')
    clean_text = clean_text.replace('&amp;', '&').replace('&quot;', '"')
    # Clean up whitespace
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()

    return clean_text
```

### Date Processing

```python
from datetime import datetime

def _extract_year(self, date_string: str) -> Optional[int]:
    """Extract year from various date formats."""
    if not date_string:
        return None

    # Try ISO format first
    try:
        dt = datetime.fromisoformat(date_string.replace('Z', '+00:00'))
        return dt.year
    except ValueError:
        pass

    # Try common patterns
    patterns = [
        r'(\d{4})-\d{2}-\d{2}',  # YYYY-MM-DD
        r'(\d{4})/\d{2}/\d{2}',  # YYYY/MM/DD
        r'(\d{4})',              # Just year
    ]

    for pattern in patterns:
        match = re.search(pattern, date_string)
        if match:
            year = int(match.group(1))
            if 1800 <= year <= 2100:  # Sanity check
                return year

    return None
```

## Error Handling

### API Errors

```python
class AudnexusError(Exception):
    """Base exception for Audnexus operations."""
    pass

class ASINNotFound(AudnexusError):
    """ASIN not found in API."""
    pass

class APIUnavailable(AudnexusError):
    """Audnexus API unavailable."""
    pass

def lookup(self, asin: str) -> Dict[str, Any]:
    """Lookup with comprehensive error handling."""
    if not self._validate_asin(asin):
        raise ValueError(f"Invalid ASIN format: {asin}")

    def _api_call():
        self.rate_limiter.acquire()

        url = f"https://api.audnex.us/books/{asin}"
        response = self.http_client.get(url)

        if response.status_code == 404:
            raise ASINNotFound(f"ASIN not found: {asin}")
        elif response.status_code != 200:
            raise APIUnavailable(f"API error: {response.status_code}")

        return response.json()

    try:
        raw_data = with_retries(_api_call, self.max_retries)
        return self._normalize_response(raw_data)
    except ASINNotFound:
        # Return empty dict for missing ASINs
        return {}
    except Exception as e:
        # Log error and return empty dict
        logger.warning(f"Audnexus lookup failed for {asin}: {e}")
        return {}
```

### Network Resilience

```python
import socket

def _is_network_available(self) -> bool:
    """Check basic network connectivity."""
    try:
        socket.create_connection(("8.8.8.8", 53), timeout=3)
        return True
    except OSError:
        return False

def lookup(self, asin: str) -> Dict[str, Any]:
    """Network-aware lookup."""
    if not self._is_network_available():
        logger.info("Network unavailable, skipping Audnexus lookup")
        return {}

    # Continue with normal lookup...
```

## Caching

### Response Caching

```python
from functools import lru_cache
import json
import hashlib

class CachedAudnexusSource(AudnexusSource):
    def __init__(self, cache_size: int = 1024, **kwargs):
        super().__init__(**kwargs)
        self.cache_size = cache_size

    @lru_cache(maxsize=1024)
    def _cached_lookup(self, asin: str) -> str:
        """Cached lookup returning JSON string."""
        result = super().lookup(asin)
        return json.dumps(result, sort_keys=True)

    def lookup(self, asin: str) -> Dict[str, Any]:
        """Lookup with memory caching."""
        cached_json = self._cached_lookup(asin)
        return json.loads(cached_json)
```

### Persistent Caching

```python
import sqlite3
from pathlib import Path

class PersistentCache:
    def __init__(self, cache_path: Path):
        self.cache_path = cache_path
        self._init_db()

    def _init_db(self):
        """Initialize cache database."""
        with sqlite3.connect(self.cache_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS audnexus_cache (
                    asin TEXT PRIMARY KEY,
                    data TEXT NOT NULL,
                    timestamp INTEGER NOT NULL
                )
            """)

    def get(self, asin: str, max_age: int = 86400) -> Optional[dict]:
        """Get cached data if fresh enough."""
        with sqlite3.connect(self.cache_path) as conn:
            cursor = conn.execute(
                "SELECT data FROM audnexus_cache WHERE asin = ? AND timestamp > ?",
                (asin, time.time() - max_age)
            )
            row = cursor.fetchone()
            return json.loads(row[0]) if row else None

    def set(self, asin: str, data: dict):
        """Cache API response."""
        with sqlite3.connect(self.cache_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO audnexus_cache (asin, data, timestamp) VALUES (?, ?, ?)",
                (asin, json.dumps(data), time.time())
            )
```

## Usage Examples

### Basic Lookup

```python
from httpx import Client

# Initialize source
client = HTTPXClient(timeout=10.0)
audnexus = AudnexusSource(
    http_client=client,
    rate_limit=5.0,
    max_retries=3
)

# Extract ASIN and lookup
asin = audnexus.extract_asin("Great Book {B001234567}")
if asin:
    metadata = audnexus.lookup(asin)
    print(f"Title: {metadata.get('title')}")
    print(f"Author: {metadata.get('author')}")
```

### Batch Processing

```python
def process_audiobook_library(library_path: Path):
    """Process entire library with rate limiting."""
    audnexus = AudnexusSource(
        http_client=HTTPXClient(),
        rate_limit=2.0  # Conservative rate
    )

    for audio_file in library_path.glob("**/*.m4b"):
        asin = audnexus.extract_asin(audio_file)
        if asin:
            metadata = audnexus.lookup(asin)
            if metadata:
                print(f"Processed: {metadata.get('title')}")
            time.sleep(0.1)  # Extra politeness
```

### Error Handling

```python
def safe_lookup(audnexus: AudnexusSource, asin: str) -> dict:
    """Lookup with comprehensive error handling."""
    try:
        return audnexus.lookup(asin)
    except ASINNotFound:
        logger.info(f"ASIN not in database: {asin}")
        return {}
    except APIUnavailable:
        logger.warning("Audnexus API temporarily unavailable")
        return {}
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return {}
```

## Testing Strategy

### Unit Tests

- ASIN extraction patterns
- Response normalization
- Error handling scenarios
- Rate limiting behavior

### Integration Tests

```python
import responses

@responses.activate
def test_api_integration():
    """Test with mocked API responses."""
    responses.add(
        responses.GET,
        "https://api.audnex.us/books/B001234567",
        json=SAMPLE_RESPONSE,
        status=200
    )

    audnexus = AudnexusSource(HTTPXClient())
    result = audnexus.lookup("B001234567")

    assert result['title'] == "The Great Audiobook: A Complete Guide"
    assert result['author'] == "John Author, Jane Coauthor"
```

## Caching Implementation

### TTL Cache for API Responses

```python
from cachetools import TTLCache, cached

# Configure cache (as recommended in 00 — Packages)
_audnexus_cache = TTLCache(maxsize=2048, ttl=60*60)  # 1 hour TTL

@cached(_audnexus_cache)
def _cached_lookup(self, asin: str) -> dict:
    """Cached API lookup to reduce redundant requests."""
    response = self.http_client.get(f"{self.base_url}/books/{asin}")
    response.raise_for_status()
    return response.json()

def lookup(self, asin: str) -> Dict[str, Any]:
    """Lookup with caching layer."""
    try:
        raw_data = self._cached_lookup(asin)
        return self._normalize_response(raw_data)
    except Exception as e:
        raise SourceUnavailable(f"Audnexus API failed: {e}")
```

### Cache Management

```python
def clear_cache(self) -> None:
    """Clear the lookup cache."""
    _audnexus_cache.clear()

def get_cache_stats(self) -> dict:
    """Get cache statistics."""
    return {
        'size': len(_audnexus_cache),
        'max_size': _audnexus_cache.maxsize,
        'hits': getattr(_audnexus_cache, 'hits', 0),
        'misses': getattr(_audnexus_cache, 'misses', 0)
    }
```

## Dependencies

### Required

- **Python 3.8+**: Type hints, dataclasses
- **One of**: httpx (preferred) OR requests

### Optional

- **httpx >= 0.27**: Preferred HTTP client
- **requests >= 2.28**: Fallback HTTP client
- **mutagen >= 1.47**: Embedded ASIN extraction
- **cachetools >= 5.3**: Response caching (recommended)
- **tenacity >= 9.0**: Retry logic with backoff

### Installation

```bash
# Preferred setup with caching
pip install httpx cachetools tenacity

# Fallback setup
pip install requests cachetools

# Full functionality
pip install httpx mutagen cachetools tenacity
```

---

## Changelog

### 2025-09-03 - Chapter Integration & Enhanced Field Support

- **Added**: Chapter integration in main extract() method with graceful error handling
- **Enhanced**: Automatic chapter fetching during metadata extraction
- **Improved**: Chapter timing accuracy using chapter-specific runtime data
- **See**: [CHANGELOG.md](./CHANGELOG.md#chapter-integration-in-audnexus-source) for implementation details

````
