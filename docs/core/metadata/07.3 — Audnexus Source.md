# 7.3) Audnexus Source (`sources/audnexus.py`)

## Overview

The Audnexus Source provides robust integration with the Audnexus API (api.audnex.us) for authoritative audiobook metadata. It handles ASIN extraction, API communication, response normalization, and error recovery.

## Features

### ASIN Extraction
- **Filename parsing**: Extract ASINs from standardized filenames
- **Embedded tag reading**: Read ASINs from audio file metadata
- **Multiple patterns**: Support various ASIN formats and positions
- **Validation**: Verify ASIN format and checksums

### API Integration
- **HTTP client abstraction**: Pluggable backend (httpx preferred/requests fallback)
- **Rate limiting**: Configurable request throttling
- **Retry logic**: Exponential backoff with jitter
- **Timeout handling**: Configurable connection/read timeouts

### Response Normalization
- **Field mapping**: API fields → internal model fields
- **Data type conversion**: String → int/date conversions
- **List processing**: Authors, narrators, genres normalization
- **HTML cleaning**: Description sanitization

## Interface

```python
class AudnexusSource:
    def __init__(
        self,
        http_client: HTTPClient,
        rate_limit: float = 5.0,
        timeout: float = 10.0,
        max_retries: int = 3
    ):
        """
        Initialize Audnexus source.

        Args:
            http_client: HTTP client implementation
            rate_limit: Max requests per second
            timeout: Request timeout in seconds
            max_retries: Max retry attempts
        """

    def lookup(self, asin: str) -> Dict[str, Any]:
        """
        Lookup audiobook metadata by ASIN.

        Args:
            asin: Amazon ASIN identifier

        Returns:
            Normalized metadata dictionary
        """

    def extract_asin(self, source: Union[Path, str]) -> Optional[str]:
        """
        Extract ASIN from filename or path.

        Args:
            source: File path or filename string

        Returns:
            Extracted ASIN or None if not found
        """
```

## ASIN Extraction

### Pattern Matching
```python
import re

ASIN_PATTERNS = [
    r'\{([A-Z0-9]{10})\}',           # {B001234567}
    r'\{([A-Z0-9]{10})\.[A-Z]{3}\}', # {B001234567.ABC}
    r'ASIN[:\s]*([A-Z0-9]{10})',     # ASIN: B001234567
    r'B00[A-Z0-9]{7}',               # B001234567 (loose)
]

def extract_asin(self, source: Union[Path, str]) -> Optional[str]:
    """Extract ASIN using multiple patterns."""
    text = str(source)

    for pattern in ASIN_PATTERNS:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            asin = match.group(1) if match.groups() else match.group(0)
            if self._validate_asin(asin):
                return asin.upper()

    return None
```

### ASIN Validation
```python
def _validate_asin(self, asin: str) -> bool:
    """Validate ASIN format and checksum."""
    if not asin or len(asin) != 10:
        return False

    # Must start with B for books
    if not asin.startswith('B'):
        return False

    # Only alphanumeric characters
    if not re.match(r'^[A-Z0-9]{10}$', asin):
        return False

    return True
```

### Embedded Tag Extraction
```python
def _extract_from_tags(self, file_path: Path) -> Optional[str]:
    """Extract ASIN from audio file tags."""
    try:
        from mutagen import File as MutagenFile

        audio = MutagenFile(file_path)
        if not audio or not audio.tags:
            return None

        # Check common ASIN tag fields
        asin_fields = ['ASIN', 'TXXX:ASIN', 'comment', 'description']

        for field in asin_fields:
            if field in audio.tags:
                value = str(audio.tags[field][0])
                asin = self.extract_asin(value)
                if asin:
                    return asin

        return None
    except ImportError:
        # Mutagen not available
        return None
    except Exception:
        # File reading error
        return None
```

## API Communication

### HTTP Client Abstraction
```python
from abc import ABC, abstractmethod

class HTTPClient(ABC):
    @abstractmethod
    def get(self, url: str, **kwargs) -> Response:
        """Perform GET request."""
        pass

class HTTPXClient(HTTPClient):
    """Preferred HTTP client using httpx."""
    def __init__(self, timeout: float = 10.0):
        import httpx
        self.client = httpx.Client(timeout=timeout)

    def get(self, url: str, **kwargs) -> httpx.Response:
        return self.client.get(url, **kwargs)

class RequestsClient(HTTPClient):
    """Fallback HTTP client using requests."""
    def __init__(self, timeout: float = 10.0):
        import requests
        self.timeout = timeout

    def get(self, url: str, **kwargs) -> requests.Response:
        import requests
        return requests.get(url, timeout=self.timeout, **kwargs)
```

### Rate Limiting
```python
import time
from threading import Lock

class RateLimiter:
    def __init__(self, max_rate: float):
        self.max_rate = max_rate  # requests per second
        self.min_interval = 1.0 / max_rate
        self.last_request = 0.0
        self.lock = Lock()

    def acquire(self):
        """Wait for rate limit clearance."""
        with self.lock:
            now = time.time()
            elapsed = now - self.last_request

            if elapsed < self.min_interval:
                sleep_time = self.min_interval - elapsed
                time.sleep(sleep_time)

            self.last_request = time.time()
```

### Retry Logic
```python
import random
from typing import Callable, TypeVar

T = TypeVar('T')

def with_retries(
    func: Callable[[], T],
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0
) -> T:
    """Execute function with exponential backoff retries."""

    for attempt in range(max_retries + 1):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries:
                raise e

            # Exponential backoff with jitter
            delay = min(base_delay * (2 ** attempt), max_delay)
            jitter = random.uniform(0.1, 0.9) * delay
            time.sleep(jitter)

    raise RuntimeError("Unreachable")
```

## API Response Handling

### Response Structure
```python
# Example Audnexus API response
SAMPLE_RESPONSE = {
    "asin": "B001234567",
    "title": "The Great Audiobook",
    "subtitle": "A Complete Guide",
    "authors": [
        {"name": "John Author"},
        {"name": "Jane Coauthor"}
    ],
    "narrators": [
        {"name": "Voice Actor"}
    ],
    "publisher": "Audio Publisher",
    "releaseDate": "2023-01-15",
    "runtime": 720,  # minutes
    "summary": "<p>Book description with <em>HTML</em></p>",
    "genres": [
        {"name": "Non-fiction"},
        {"name": "Self-help"}
    ],
    "series": [
        {
            "name": "Great Series",
            "position": "1"
        }
    ]
}
```

### Field Normalization
```python
def _normalize_response(self, raw_data: dict) -> Dict[str, Any]:
    """Normalize API response to internal format."""
    normalized = {}

    # Basic string fields
    normalized['title'] = raw_data.get('title', '')
    normalized['asin'] = raw_data.get('asin', '')
    normalized['publisher'] = raw_data.get('publisher', '')

    # Subtitle handling
    subtitle = raw_data.get('subtitle', '')
    if subtitle:
        normalized['title'] = f"{normalized['title']}: {subtitle}"

    # Author list processing
    authors = raw_data.get('authors', [])
    if authors:
        author_names = [author.get('name', '') for author in authors]
        normalized['author'] = ', '.join(filter(None, author_names))

    # Narrator list processing
    narrators = raw_data.get('narrators', [])
    if narrators:
        narrator_names = [narrator.get('name', '') for narrator in narrators]
        normalized['narrator'] = ', '.join(filter(None, narrator_names))

    # Date processing
    release_date = raw_data.get('releaseDate', '')
    if release_date:
        normalized['year'] = self._extract_year(release_date)

    # Duration conversion (minutes to seconds)
    runtime = raw_data.get('runtime')
    if runtime:
        normalized['duration_sec'] = int(runtime) * 60

    # HTML description cleaning
    summary = raw_data.get('summary', '')
    if summary:
        normalized['description'] = self._clean_html(summary)

    # Genre list processing
    genres = raw_data.get('genres', [])
    if genres:
        genre_names = [genre.get('name', '') for genre in genres]
        normalized['genres'] = list(filter(None, genre_names))

    # Series information
    series_list = raw_data.get('series', [])
    if series_list:
        series_info = series_list[0]  # Take first series
        normalized['series'] = series_info.get('name', '')
        position = series_info.get('position', '')
        if position:
            normalized['volume'] = str(position).zfill(2)

    return normalized
```

### Date Processing
```python
from datetime import datetime

def _extract_year(self, date_string: str) -> Optional[int]:
    """Extract year from various date formats."""
    if not date_string:
        return None

    # Try ISO format first
    try:
        dt = datetime.fromisoformat(date_string.replace('Z', '+00:00'))
        return dt.year
    except ValueError:
        pass

    # Try common patterns
    patterns = [
        r'(\d{4})-\d{2}-\d{2}',  # YYYY-MM-DD
        r'(\d{4})/\d{2}/\d{2}',  # YYYY/MM/DD
        r'(\d{4})',              # Just year
    ]

    for pattern in patterns:
        match = re.search(pattern, date_string)
        if match:
            year = int(match.group(1))
            if 1800 <= year <= 2100:  # Sanity check
                return year

    return None
```

## Error Handling

### API Errors
```python
class AudnexusError(Exception):
    """Base exception for Audnexus operations."""
    pass

class ASINNotFound(AudnexusError):
    """ASIN not found in API."""
    pass

class APIUnavailable(AudnexusError):
    """Audnexus API unavailable."""
    pass

def lookup(self, asin: str) -> Dict[str, Any]:
    """Lookup with comprehensive error handling."""
    if not self._validate_asin(asin):
        raise ValueError(f"Invalid ASIN format: {asin}")

    def _api_call():
        self.rate_limiter.acquire()

        url = f"https://api.audnex.us/books/{asin}"
        response = self.http_client.get(url)

        if response.status_code == 404:
            raise ASINNotFound(f"ASIN not found: {asin}")
        elif response.status_code != 200:
            raise APIUnavailable(f"API error: {response.status_code}")

        return response.json()

    try:
        raw_data = with_retries(_api_call, self.max_retries)
        return self._normalize_response(raw_data)
    except ASINNotFound:
        # Return empty dict for missing ASINs
        return {}
    except Exception as e:
        # Log error and return empty dict
        logger.warning(f"Audnexus lookup failed for {asin}: {e}")
        return {}
```

### Network Resilience
```python
import socket

def _is_network_available(self) -> bool:
    """Check basic network connectivity."""
    try:
        socket.create_connection(("8.8.8.8", 53), timeout=3)
        return True
    except OSError:
        return False

def lookup(self, asin: str) -> Dict[str, Any]:
    """Network-aware lookup."""
    if not self._is_network_available():
        logger.info("Network unavailable, skipping Audnexus lookup")
        return {}

    # Continue with normal lookup...
```

## Caching

### Response Caching
```python
from functools import lru_cache
import json
import hashlib

class CachedAudnexusSource(AudnexusSource):
    def __init__(self, cache_size: int = 1024, **kwargs):
        super().__init__(**kwargs)
        self.cache_size = cache_size

    @lru_cache(maxsize=1024)
    def _cached_lookup(self, asin: str) -> str:
        """Cached lookup returning JSON string."""
        result = super().lookup(asin)
        return json.dumps(result, sort_keys=True)

    def lookup(self, asin: str) -> Dict[str, Any]:
        """Lookup with memory caching."""
        cached_json = self._cached_lookup(asin)
        return json.loads(cached_json)
```

### Persistent Caching
```python
import sqlite3
from pathlib import Path

class PersistentCache:
    def __init__(self, cache_path: Path):
        self.cache_path = cache_path
        self._init_db()

    def _init_db(self):
        """Initialize cache database."""
        with sqlite3.connect(self.cache_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS audnexus_cache (
                    asin TEXT PRIMARY KEY,
                    data TEXT NOT NULL,
                    timestamp INTEGER NOT NULL
                )
            """)

    def get(self, asin: str, max_age: int = 86400) -> Optional[dict]:
        """Get cached data if fresh enough."""
        with sqlite3.connect(self.cache_path) as conn:
            cursor = conn.execute(
                "SELECT data FROM audnexus_cache WHERE asin = ? AND timestamp > ?",
                (asin, time.time() - max_age)
            )
            row = cursor.fetchone()
            return json.loads(row[0]) if row else None

    def set(self, asin: str, data: dict):
        """Cache API response."""
        with sqlite3.connect(self.cache_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO audnexus_cache (asin, data, timestamp) VALUES (?, ?, ?)",
                (asin, json.dumps(data), time.time())
            )
```

## Usage Examples

### Basic Lookup
```python
from httpx import Client

# Initialize source
client = HTTPXClient(timeout=10.0)
audnexus = AudnexusSource(
    http_client=client,
    rate_limit=5.0,
    max_retries=3
)

# Extract ASIN and lookup
asin = audnexus.extract_asin("Great Book {B001234567}")
if asin:
    metadata = audnexus.lookup(asin)
    print(f"Title: {metadata.get('title')}")
    print(f"Author: {metadata.get('author')}")
```

### Batch Processing
```python
def process_audiobook_library(library_path: Path):
    """Process entire library with rate limiting."""
    audnexus = AudnexusSource(
        http_client=HTTPXClient(),
        rate_limit=2.0  # Conservative rate
    )

    for audio_file in library_path.glob("**/*.m4b"):
        asin = audnexus.extract_asin(audio_file)
        if asin:
            metadata = audnexus.lookup(asin)
            if metadata:
                print(f"Processed: {metadata.get('title')}")
            time.sleep(0.1)  # Extra politeness
```

### Error Handling
```python
def safe_lookup(audnexus: AudnexusSource, asin: str) -> dict:
    """Lookup with comprehensive error handling."""
    try:
        return audnexus.lookup(asin)
    except ASINNotFound:
        logger.info(f"ASIN not in database: {asin}")
        return {}
    except APIUnavailable:
        logger.warning("Audnexus API temporarily unavailable")
        return {}
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return {}
```

## Testing Strategy

### Unit Tests
- ASIN extraction patterns
- Response normalization
- Error handling scenarios
- Rate limiting behavior

### Integration Tests
```python
import responses

@responses.activate
def test_api_integration():
    """Test with mocked API responses."""
    responses.add(
        responses.GET,
        "https://api.audnex.us/books/B001234567",
        json=SAMPLE_RESPONSE,
        status=200
    )

    audnexus = AudnexusSource(HTTPXClient())
    result = audnexus.lookup("B001234567")

    assert result['title'] == "The Great Audiobook: A Complete Guide"
    assert result['author'] == "John Author, Jane Coauthor"
```

## Caching Implementation

### TTL Cache for API Responses
```python
from cachetools import TTLCache, cached

# Configure cache (as recommended in 00 — Packages)
_audnexus_cache = TTLCache(maxsize=2048, ttl=60*60)  # 1 hour TTL

@cached(_audnexus_cache)
def _cached_lookup(self, asin: str) -> dict:
    """Cached API lookup to reduce redundant requests."""
    response = self.http_client.get(f"{self.base_url}/books/{asin}")
    response.raise_for_status()
    return response.json()

def lookup(self, asin: str) -> Dict[str, Any]:
    """Lookup with caching layer."""
    try:
        raw_data = self._cached_lookup(asin)
        return self._normalize_response(raw_data)
    except Exception as e:
        raise SourceUnavailable(f"Audnexus API failed: {e}")
```

### Cache Management
```python
def clear_cache(self) -> None:
    """Clear the lookup cache."""
    _audnexus_cache.clear()

def get_cache_stats(self) -> dict:
    """Get cache statistics."""
    return {
        'size': len(_audnexus_cache),
        'max_size': _audnexus_cache.maxsize,
        'hits': getattr(_audnexus_cache, 'hits', 0),
        'misses': getattr(_audnexus_cache, 'misses', 0)
    }
```

## Dependencies

### Required
- **Python 3.8+**: Type hints, dataclasses
- **One of**: httpx (preferred) OR requests

### Optional
- **httpx >= 0.27**: Preferred HTTP client
- **requests >= 2.28**: Fallback HTTP client
- **mutagen >= 1.47**: Embedded ASIN extraction
- **cachetools >= 5.3**: Response caching (recommended)
- **tenacity >= 9.0**: Retry logic with backoff

### Installation
```bash
# Preferred setup with caching
pip install httpx cachetools tenacity

# Fallback setup
pip install requests cachetools

# Full functionality
pip install httpx mutagen cachetools tenacity
```
